%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Definition of Conditional PMF}
\framesubtitle{How does the distribution of $y$ change with $x$?}

\begin{eqnarray*}
	p_{Y|X}(y|x) = P(Y=y|X=x) =  \frac{P(Y=y \cap X=x)}{P(X=x)} =  \frac{p_{XY}(x,y)}{p_X(x)}
\end{eqnarray*}
\vspace{1em}

%Hence,
% $$\boxed{p_{Y|X}(y|x) = \frac{p_{XY}(x,y)}{p_X(x)}}$$
% and similarly,
%  $$\boxed{p_{X|Y}(x|y) = \frac{p_{XY}(x,y)}{p_Y(y)}}$$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional Probability Mass Function \& Independence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Conditional PMF of $Y$ given $X = 2$}

\begin{table}
\begin{tabular}{|cc|ccc|c|}
\hline
&&\multicolumn{3}{c|}{$Y$}&\\
&&1 & 2&3&\\
\hline
\multirow{4}{*}{$X$}
&0& \multicolumn{1}{|c}{1/8} & 0& 0&1/8\\
&1& \multicolumn{1}{|c}{0} & 1/4&1/8&3/8\\
&2& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&3& \multicolumn{1}{|c}{1/8} & 0&0&1/8\\
\hline
\end{tabular}
\end{table}

\small
\begin{eqnarray*}
	p_{Y|X}(1|2) &=& \frac{p_{XY}(2,1)}{p_X(2)} = \frac{0}{3/8} =\alert{0}\\ \\
	p_{Y|X}(2|2) &=&\frac{p_{XY}(2,2)}{p_X(2)}= \frac{1/4}{3/8} =\alert{2/3} \\ \\ 
	 p_{Y|X}(3|2) &=&\frac{p_{XY}(2,3)}{p_X(2)} = \frac{1/8}{3/8} = \alert{1/3}
\end{eqnarray*}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{What is $p_{X|Y}(1|2)$? \hfill \includegraphics[scale = 0.05]{./images/clicker}}
\small
\begin{table}
\begin{tabular}{|cc|ccc|c|}
\hline
&&\multicolumn{3}{c|}{$Y$}&\\
&&1 & 2&3&\\
\hline
\multirow{4}{*}{$X$}
&0& \multicolumn{1}{|c}{1/8} & \alert{0}&0&\\
&1& \multicolumn{1}{|c}{0} & \alert{1/4}&1/8&\\
&2& \multicolumn{1}{|c}{0} & \alert{1/4}&1/8&\\
&3& \multicolumn{1}{|c}{1/8} & \alert{0}&0&\\
\hline 
&&1/4&\textcolor{blue}{1/2}&1/4&\\
\hline
\end{tabular}
\end{table}

\pause
\begin{equation*}
	p_{X|Y}(1|2) =\frac{p_{XY}(1,2)}{p_Y(2)}=\frac{1/4}{1/2}= \alert{1/2}
\end{equation*}

  \pause

Similarly: 
$$p_{X|Y}(0|2)=0, \quad p_{X|Y}(2|2)=1/2, \quad p_{X|Y}(3|2)= 0$$


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Independent RVs: Joint Equals Product of Marginals}


\begin{block}{Definition}
  Two discrete RVs are \alert{independent} if and only if
  $$p_{XY}(x,y) = p_X(x)p_Y(y)$$ for all pairs $(x,y)$ in the support.
\end{block}


\begin{block}{Equivalent Definition}
  \vspace{-1em}
  $$p_{Y|X}(y|x) = p_Y(y) \mbox{ and } p_{X|Y}(x|y) = p_X(x)$$
  for all pairs $(x,y)$ in the support.
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Are $X$ and $Y$ Independent? \hfill \includegraphics[scale = 0.05]{./images/clicker}}
\alert{(A = YES, B = NO)}
\small
\begin{table}
\begin{tabular}{|cc|ccc|c|}
\hline
&&\multicolumn{3}{c|}{$Y$}&\\
&&1 & 2&3&\\
\hline
\multirow{4}{*}{$X$}
&0& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}& \alert{0}&\textcolor{blue}{1/8}\\
&1& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&2& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&3& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}&\alert{0}&\textcolor{blue}{1/8}\\
\hline
&&\textcolor{blue}{1/4}&\textcolor{blue}{1/2}&\textcolor{blue}{1/4}&\\
\hline
\end{tabular}
\end{table}

\pause

\begin{eqnarray*}
	p_{XY}(2,1) &=& 0\\ 
	p_X(2) \times p_Y(1) &=&  (3/8) \times (1/4) \neq 0
\end{eqnarray*}

\alert{Therefore $X$ and $Y$ are \emph{not} independent.}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conditional Expectation \& The Law of Iterated Expectations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Conditional Expectation}
\begin{block}{Intuition}
	$E[Y|X] = $ ``best guess'' of realization that $Y$ after observing realization of $X$. 
\end{block}

\begin{block}{$E[Y|X]$ is a Random Variable}
While $E[Y]$ is a constant, $E[Y|X]$ is a function of $X$, hence a \alert{Random Variable}.
\end{block}

\begin{block}{$E[Y|X=x]$ is a Constant}
  The constant $E[Y|X=x]$ is the ``guess'' of $Y$ if we see $X=x$.
\end{block}

\begin{block}{Calculating $E[Y|X=x]$}
Take the mean of the conditional pmf of $Y$ given $X=x$.
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Conditional Expectation: $E[Y|X=2]$}

\footnotesize
\begin{table}
\begin{tabular}{|cc|ccc|c|}
\hline
&&\multicolumn{3}{c|}{$Y$}&\\
&&1 & 2&3&\\
\hline
\multirow{4}{*}{$X$}
&0& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}& \alert{0}&\textcolor{blue}{1/8}\\
&1& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&2& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&3& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}&\alert{0}&\textcolor{blue}{1/8}\\
\hline
&&\textcolor{blue}{1/4}&\textcolor{blue}{1/2}&\textcolor{blue}{1/4}&\\
\hline
\end{tabular}
\end{table}

We showed above that the conditional pmf of $Y|X=2$ is:
	$$\boxed{\begin{array}{ccc}p_{Y|X}(1|2) =0 \quad&p_{Y|X}(2|2) =2/3 \quad&p_{Y|X}(3|2) =1/3\end{array}}$$
	
	
Hence
	$$E[Y|X=2] = 2 \times 2/3 + 3 \times 1/3 =  \alert{7/3}$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Conditional Expectation: $E[Y|X=0]$}

\footnotesize
\begin{table}
\begin{tabular}{|cc|ccc|c|}
\hline
&&\multicolumn{3}{c|}{$Y$}&\\
&&1 & 2&3&\\
\hline
\multirow{4}{*}{$X$}
&0& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}& \alert{0}&\textcolor{blue}{1/8}\\
&1& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&2& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&3& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}&\alert{0}&\textcolor{blue}{1/8}\\
\hline
&&\textcolor{blue}{1/4}&\textcolor{blue}{1/2}&\textcolor{blue}{1/4}&\\
\hline
\end{tabular}
\end{table}

The conditional pmf of $Y|X=0$ is 
	$$\boxed{\begin{array}{ccc}p_{Y|X}(1|0) =1 \quad&p_{Y|X}(2|0) =0 \quad&p_{Y|X}(3|0) =0\end{array}}$$
	

\alert{Hence $E[Y|X=0] = 1$}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Calculate $E[Y|X=3]$}

\footnotesize
\begin{table}
\begin{tabular}{|cc|ccc|c|}
\hline
&&\multicolumn{3}{c|}{$Y$}&\\
&&1 & 2&3&\\
\hline
\multirow{4}{*}{$X$}
&0& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}& \alert{0}&\textcolor{blue}{1/8}\\
&1& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&2& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&3& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}&\alert{0}&\textcolor{blue}{1/8}\\
\hline
&&\textcolor{blue}{1/4}&\textcolor{blue}{1/2}&\textcolor{blue}{1/4}&\\
\hline
\end{tabular}
\end{table}


The conditional pmf of $Y|X=3$ is
	$$\boxed{\begin{array}{ccc}p_{Y|X}(1|3) =1 \quad&p_{Y|X}(2|3) =0 \quad&p_{Y|X}(3|3) =0\end{array}}$$
	
\alert{Hence $E[Y|X=3] = 1$}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Calculate $E[Y|X=1]$ \hfill \includegraphics[scale = 0.05]{./images/clicker}}

\footnotesize
\begin{table}
\begin{tabular}{|cc|ccc|c|}
\hline
&&\multicolumn{3}{c|}{$Y$}&\\
&&1 & 2&3&\\
\hline
\multirow{4}{*}{$X$}
&0& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}& \alert{0}&\textcolor{blue}{1/8}\\
&1& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&2& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&3& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}&\alert{0}&\textcolor{blue}{1/8}\\
\hline
&&\textcolor{blue}{1/4}&\textcolor{blue}{1/2}&\textcolor{blue}{1/4}&\\
\hline
\end{tabular}
\end{table}
\pause
The conditional pmf of $Y|X=1$ is
	$$\boxed{\begin{array}{ccc}p_{Y|X}(1|1) =0 \quad&p_{Y|X}(2|1) =2/3 \quad&p_{Y|X}(3|1) =1/3\end{array}}$$
	

Hence
	$$E[Y|X=1] = 2 \times 2/3 + 3 \times 1/3 = \alert{7/3}$$

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{$E[Y|X]$ is a Random Variable}
For this example:
	$$E[Y|X]= \left\{\begin{array}{cc}  
	1& X = 0\\
	7/3& X = 1\\
	7/3& X = 2\\ 
	1& X = 3 
	\end{array}\right.$$ 
From above the marginal distribution of $X$ is:
	\begin{eqnarray*}
		P(X=0)= 1/8 && P(X=1) = 3/8\\
		P(X=2)=3/8 && P(X=3)=1/8
	\end{eqnarray*} 
\alert{$E[Y|X]$ takes the value 1 with prob.\ 1/4 and 7/3 with prob.\ 3/4.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The Law of Iterated Expectations}
\framesubtitle{$E[Y|X]$ is an RV so what is its expectation?}
For any RVs $X$ and $Y$
	$$\alert{E\left[E\left[Y|X  \right]  \right] = E[Y]}$$
Option proof \textcolor{blue}{\href{http://ditraglia.com/Econ103Public/OptionalProofs.pdf}{\fbox{HERE}}}.
(Helpful for Econ 104\dots)


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t]
\frametitle{Law of Iterated Expectations for Our Example}

\begin{columns}
\footnotesize
\column{0.5\textwidth}
\begin{block}{Marginal pmf of $Y$}
\begin{eqnarray*}
	P(Y = 1) &=& 1/4 \\
	P(Y = 2) &=& 1/2\\
	P(Y = 3) &=& 1/4
\end{eqnarray*}


\begin{eqnarray*}
	E[Y] &=& 1\times 1/4 + 2 \times 1/2 + 3 \times 1/4\\
		&=&2
\end{eqnarray*}
\end{block}

\column{0.5\textwidth}
\begin{block}{$E[Y|X]$}
	\begin{eqnarray*}
	 E[Y|X] &=& \left\{\begin{array}{cc} 1& \mbox{w/ prob. } 1/4\\ 7/3& \mbox{w/ prob. } 3/4\end{array}\right.\\\\ 
	 E\left[E\left[Y|X \right] \right] &=& 1 \times 1/4 + 7/3 \times 3/4\\ 
	 &=& 2
	\end{eqnarray*}
	\vspace{1em}
\end{block}

\end{columns}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expectation of a Function of Two Discrete RVs, Covariance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Expectation of Function of Two Discrete RVs}
\Large
		$$\boxed{E[g(X,Y)] = \sum_x\sum_y g(x,y)p_{XY}(x,y)}$$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Some Extremely Important Examples}
\framesubtitle{Same For Continuous Random Variables}
Let $\mu_X = E[X], \mu_Y = E[Y]$
\vspace{2em}

\begin{block}{Covariance}
$$\sigma_{XY} = Cov(X,Y) = E[(X-\mu_X)(Y - \mu_Y)]$$
\end{block}

\begin{block}{Correlation}
$$\rho_{XY} = Corr(X,Y) = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$$
\end{block}
\vspace{3em}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Shortcut Formula for Covariance}

Much easier for calculating:
 $$\boxed{Cov(X,Y) = E[XY] - E[X]E[Y]}$$


\vspace{2em}
I'll mention this again in a few slides\dots
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Calculating $Cov(X,Y)$}

\begin{columns}

\column{0.42\textwidth}
\begin{table}
\footnotesize
\begin{tabular}{|cc|ccc|c|}
\hline
&&\multicolumn{3}{c|}{$Y$}&\\
&&1 & 2&3&\\
\hline
\multirow{4}{*}{$X$}
&0& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}& \alert{0}&\textcolor{blue}{1/8}\\
&1& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&2& \multicolumn{1}{|c}{\alert{0}} & \alert{1/4}&\alert{1/8}&\textcolor{blue}{3/8}\\
&3& \multicolumn{1}{|c}{\alert{1/8}} & \alert{0}&\alert{0}&\textcolor{blue}{1/8}\\
\hline
&&\textcolor{blue}{1/4}&\textcolor{blue}{1/2}&\textcolor{blue}{1/4}&\\
\hline
\end{tabular}
\vspace{6em}
\end{table}


\column{0.65\textwidth}
\footnotesize
\begin{eqnarray*}
	E[X] &=&3/8 + 2 \times 3/8 + 3 \times 1/8 = 3/2\\\\ 
	E[Y] &=&1/4 + 2 \times 1/2 + 3 \times 1/4 = 2 \\ \\ \pause
	E[XY] &=& 1/4\times ( 2 + 4) + 1/8 \times (3 + 6 + 3)\\ 
		&=& 3\\ \\ \pause
	Cov(X,Y) &=& E[XY] - E[X]E[Y]\\ \pause
			&=& 3 - 3/2\times 2 = 0\\\\ \pause
	Corr(X,Y) &=& Cov(X,Y)/\left[SD(X) SD(Y) \right] = 0
\end{eqnarray*}

\end{columns}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Zero Covariance versus Independence}
  \begin{itemize}
    \item From this example we learn that zero covariance (correlation) \emph{does not} imply independence.
    \item However, it turns out that independence \emph{does} imply zero covariance (correlation).
  \end{itemize}

\vspace{2em}
\normalsize 
Optional proof that independence implies zero covariance \textcolor{blue}{\href{http://ditraglia.com/Econ103Public/OptionalProofs.pdf}{\fbox{HERE}}}.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linearity of Expectation Reprise, Properties of Binomial RV}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Linearity of Expectation, Again}
\framesubtitle{Holds for Continuous RVs as well, but different proof.}
In general, $E[g(X,Y)]\neq g(E[X],E[Y])$. The key exception is when $g$ is a linear function:

\Large
$$\boxed{E[aX + bY + c] = aE[X] + bE[Y] + c}$$

\normalsize
where $X,Y$ are random variables and $a,b,c$ are constants.
Optional proof \textcolor{blue}{\href{http://ditraglia.com/Econ103Public/OptionalProofs.pdf}{\fbox{HERE}}}.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Application: Shortcut Formula for Variance}

By the Linearity of Expectation, 
\begin{eqnarray*}
	Var(X) &=&  E[(X - \mu)^2] = \pause E[X^2 - 2\mu X + \mu^2]\\
		&=& \pause E[X^2] - 2\mu E[X] + \mu^2\\
		&=& \pause E[X^2] - 2\mu^2 + \mu^2\\
		&=& \pause E[X^2] - \mu^2
\end{eqnarray*}

\alert{We saw in a previous lecture that it's typically much easier to calculate variances using the shortcut formula.}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Another Application: Shortcut Formula for Covariance}
\alert{Similar to Shortcut for Variance: in fact $Var(X) = Cov(X,X)$}
\begin{eqnarray*}
	Cov(X,Y)&=& E[(X - \mu_X)(Y-\mu_Y)]\\
			&=& E[XY - \mu_X Y - \mu_Y X + \mu_X \mu_Y]\\
			&\vdots& \\
			%&=&E[XY] - \mu_xE[Y] - \mu_Y E[X] + \mu_X \mu_Y\\
			%&=& E[XY] - \mu_X\mu_Y - \mu_Y\mu_X + \mu_X \mu_Y\\
			%&=& E[XY] - \mu_X \mu_Y\\
			&=& E[XY] - E[X]E[Y]
\end{eqnarray*}

\hfill \alert{You'll fill in the details for homework...}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Expected Value of Sum = Sum of Expected Values}
Repeatedly applying the linearity of expectation,
$$E[X_1 + X_2 + \hdots + X_n] = E[X_1] + E[X_2] + \hdots + E[X_n]$$
regardless of how the RVs $X_1, \hdots, X_n$ are related to each other. In particular it \alert{doesn't matter if they're dependent or independent}.


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Independent and Identically Distributed (iid) RVs}

\begin{block}{Example}
	$X_1, X_2, \hdots X_n \sim \mbox{iid Bernoulli}(p)$
\end{block}

\begin{block}{Independent}
Realization of one of the RVs gives no information about the others.
\end{block}

\begin{block}{Identically Distributed}
Each $X_i$ is the same kind of RV, with the same values for any parameters. (Hence same pmf, cdf, mean, variance, etc.)
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Recall: Binomial$(n,p)$ Random Variable}

\begin{block}{Definition}
Sum of $n$ independent Bernoulli RVs, each with probability of ``success,'' i.e.\ 1, equal to $p$
\end{block}


\begin{alertblock}{Using Our New Notation}
Let $X_1, X_2, \hdots, X_n \sim \mbox{iid Bernoulli}(p)$, $Y = X_1 + X_2 + \hdots + X_n$. Then $Y \sim \mbox{Binomial}(n,p)$.
\end{alertblock}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Which of these is the PMF of a Binomial$(n,p)$ RV? \hfill \includegraphics[scale = 0.05]{./images/clicker}}
%
%\begin{enumerate}[(a)]
%	\item $p(x) = p^x (1-p)^{n-x}$
%	\item $p(x) = {n \choose x} p^x (1-p)^{n-x}$
%	\item $p(x) = {x \choose n} p^x$
%	\item $p(x) = {n \choose x} p^{n-x} (1-p)^{x}$
%	\item $p(x) = p^n (1-p)^{x}$
%\end{enumerate}
%
%\pause
% 
%$$\alert{p(x) = {n \choose x} p^x (1-p)^{n-x}}$$ 
%
%
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Expected Value of Binomial RV}

Use the fact that a Binomial$(n,p)$ RV is defined as the sum of $n$ iid Bernoulli$(p)$ Random Variables and the Linearity of Expectation:
\begin{eqnarray*}
E[Y] &=& E[X_1 + X_2 + \hdots + X_n] =  E[X_1] + E[X_2] + \hdots +E[X_n]\\
	&=& p + p + \hdots + p\\
	&=&  \alert{np}
\end{eqnarray*}
\vspace{3em}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Variance of a Sum $\neq$ Sum of Variances!}
\footnotesize
\begin{eqnarray*}
	Var(aX + bY) &=& E\left[\left\{\left( aX + bY\right) - E[aX + bY]\right\}^2  \right]\\ \pause
	&=& E\left[  \left\{a (X - \mu_X) + b(Y - \mu_Y)\right\}^2 \right]\\ \pause
	&=& E\left[a^2(X-\mu_X)^2 + b^2(Y-\mu_Y)^2 + 2ab(X-\mu_X)(Y-\mu_Y)  \right]\\ \pause
	&=&a^2E[(X-\mu_X)^2] + b^2 E[(Y-\mu_Y)^2] + 2ab E[(X-\mu_X)(Y-\mu_Y)]\\ \pause
	&=& \alert{a^2 Var(X) + b^2 Var(Y) + 2ab Cov(X,Y)}
\end{eqnarray*}


\vspace{3em}
\normalsize
Since $\sigma_{XY} = \rho\sigma_X \sigma_Y$, this is sometimes written as:
$$\alert{\boxed{Var(aX + bY) = a^2 \sigma_X^2 + b^2 \sigma_Y^2 + 2ab \rho \sigma_X \sigma_Y}}$$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Independence $\Rightarrow Var(X+Y) = Var(X) + Var(Y)$}

$X$ and $Y$ independent $\implies Cov(X,Y)=0$. Hence, independence implies
\begin{eqnarray*}
	Var(X + Y) &=& Var(X) + Var(Y) + 2 Cov(X,Y)\\
			&=& \alert{Var(X) +Var(Y)}
\end{eqnarray*}


\begin{block}{Also true for three or more RVs}
If $X_1, X_2, \hdots, X_n$ are independent, then
	$$Var(X_1 + X_2 + \hdots X_n) = Var(X_1) + Var(X_2) + \hdots + Var(X_n)$$
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Crucial Distinction}
\begin{block}{Expected Value}
\alert{Always} true that
	$$E[X_1 + X_2 + \hdots + X_n] = E[X_1] + E[X_2] + \hdots + E[X_n]$$
\end{block}


\begin{block}{Variance}
\alert{Not true in general} that 
	$$Var[X_1 + X_2 + \hdots + X_n] = Var[X_1] + Var[X_2] + \hdots + Var[X_n]$$
except in the special case where $X_1, \hdots X_n$ are independent (or at least uncorrelated).
\end{block}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Variance of Binomial Random Variable}
\begin{block}{Definition from Sequence of Bernoulli Trials}
If $X_1, X_2, \hdots, X_n \sim \mbox{iid Bernoulli}(p)$ then 
	$$Y = X_1 + X_2 + \hdots + X_n \sim \mbox{Binomial}(n,p)$$
\end{block}

\alert{Using Independence}
\begin{eqnarray*}
Var[Y] &=&  Var[X_1 + X_2 + \hdots + X_n]\\
	&=& Var[X_1] + Var[X_2] + \hdots +Var[X_n]\\
	&=&\pause p(1-p) + p(1-p) + \hdots + p(1-p)\\
	&=& \pause \alert{np(1-p)}
\end{eqnarray*}
\vspace{3em}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
